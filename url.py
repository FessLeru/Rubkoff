import requests
from bs4 import BeautifulSoup
import json
import time

BASE_URL = "https://rubkoff.ru"


def get_soup(url):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç BeautifulSoup"""
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers, timeout=10)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")


def get_project_links():
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–µ–∫—Ç—ã"""
    print("üîç –ó–∞–≥—Ä—É–∂–∞—é —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤...")
    soup = get_soup(f"{BASE_URL}/nashi-raboty/")
    links = []
    for a in soup.select("a.product-card, a.project-card"):
        href = a.get("href")
        if href and href.startswith("/nashi-raboty/") and href not in links:
            links.append(BASE_URL + href)
    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(links)} –ø—Ä–æ–µ–∫—Ç–æ–≤.")
    return links


def parse_project(url):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞"""
    print(f"üì∏ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {url}")
    soup = get_soup(url)

    # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫ "–ü–æ—Ö–æ–∂–∏–µ –ø—Ä–æ–µ–∫—Ç—ã"
    for bad in soup.find_all(
        lambda tag: tag.name == "section" and (
            "similar" in " ".join(tag.get("class", [])) or
            "related" in " ".join(tag.get("class", [])) or
            "–ü–æ—Ö–æ–∂–∏–µ –ø—Ä–æ–µ–∫—Ç—ã" in tag.get_text()
        )
    ):
        bad.decompose()

    # –ù–∞–∑–≤–∞–Ω–∏–µ
    title = soup.select_one("h1")
    title = title.get_text(strip=True) if title else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

    # –§–æ—Ç–æ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ª–∞–π–¥–µ—Ä–∞
    images = []
    gallery = soup.select_one(".product-gallery, .swiper, .product-photos, .project-gallery")
    if gallery:
        for img in gallery.select("img"):
            src = img.get("data-src") or img.get("src")
            if src and "upload" in src:
                if not src.startswith("http"):
                    src = BASE_URL + src
                if src not in images:
                    images.append(src)

    # --- –ù–û–í–û–ï: –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –±–ª–æ–∫–∞ .project-desc-text ---
    desc_block = soup.select_one(".project-desc-text")
    description = ""
    if desc_block:
        for el in desc_block.find_all(["h4", "p"]):
            text = el.get_text(" ", strip=True)
            if text and text.lower() != "–æ –ø—Ä–æ–µ–∫—Ç–µ":
                description += text + "\n"

    # --- –ù–û–í–û–ï: –¢–∞–±–ª–∏—Ü–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ .desc-table__chars ---
    characteristics = {}
    table = soup.select_one("table.desc-table__chars")
    if table:
        for tr in table.select("tr"):
            tds = tr.select("td")
            if len(tds) == 2:
                key = tds[0].get_text(strip=True)
                val = tds[1].get_text(strip=True)
                if key:
                    characteristics[key] = val

    return {
        "url": url,
        "title": title,
        "description": description.strip(),
        "characteristics": characteristics,
        "images": images
    }


def main():
    projects_data = []
    links = get_project_links()

    for i, link in enumerate(links, start=1):
        try:
            print(f"[{i}/{len(links)}]")
            data = parse_project(link)
            projects_data.append(data)
            time.sleep(1)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link}: {e}")

    with open("rubkoff_projects.json", "w", encoding="utf-8") as f:
        json.dump(projects_data, f, ensure_ascii=False, indent=2)

    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ rubkoff_projects.json")


if __name__ == "__main__":
    main()
